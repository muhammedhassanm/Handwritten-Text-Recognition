{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating OCR Models\n",
    "### TODO - Finish abstract class of cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Segmantation model:\n",
      "INFO:tensorflow:Restoring parameters from models/gap-clas/CNN-CG\n",
      "INFO:tensorflow:Restoring parameters from models/gap-clas/RNN/Bi-RNN-new\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "#from abc import ABC, abstractmethod\n",
    "\n",
    "# Import Widgets\n",
    "from ipywidgets import Button, Text, HBox, VBox\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Import costume functions, corresponding to notebooks\n",
    "from ocr import charSeg\n",
    "from ocr.normalization import letterNorm, imageNorm\n",
    "# from ocr import charSeg\n",
    "# Helpers\n",
    "from ocr.helpers import implt, resize, extendImg\n",
    "from ocr.datahelpers import loadWordsData, idx2char\n",
    "from ocr.tfhelpers import Graph\n",
    "from ocr.viz import printProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/char-clas/en/CharClassifier\n",
      "INFO:tensorflow:Restoring parameters from models/word-clas/en/WordClassifier2\n",
      "INFO:tensorflow:Restoring parameters from models/word-clas/en/CTC/Classifier2\n"
     ]
    }
   ],
   "source": [
    "charClass_1 = Graph('models/char-clas/' + LANG + '/CharClassifier')\n",
    "# charClass_2 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_2', 'prediction')\n",
    "# charClass_3 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_1', 'prediction')\n",
    "\n",
    "wordClass = Graph('models/word-clas/' + LANG + '/WordClassifier2', 'prediction_infer')\n",
    "#wordClass2 = Graph('models/word-clas/' + LANG + '/SeqRNN/Classifier3', 'word_prediction') # None\n",
    "wordClass3 = Graph('models/word-clas/' + LANG + '/CTC/Classifier2', 'word_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      "('-> Number of words:', 267)\n",
      " |****************************************| 100.0% \n",
      "()\n",
      "('Number of chars:', 1356)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piyush_Jena/ai-saturdays/tf/lib/python2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'numpy.string_'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    }
   ],
   "source": [
    "images, labels = loadWordsData('data/test_words/' + LANG + '_raw', loadGaplines=False)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    printProgressBar(i, len(images))\n",
    "    images[i] = imageNorm(\n",
    "        cv2.cvtColor(images[i], cv2.COLOR_GRAY2RGB),\n",
    "        60,\n",
    "        border=False,\n",
    "        tilt=True,\n",
    "        hystNorm=True)\n",
    "\n",
    "if LANG == 'en':\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = unidecode.unidecode(labels[i])\n",
    "print()        \n",
    "print('Number of chars:', sum(len(l) for l in labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xce in position 1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba806d4a814b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLANG\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Piyush_Jena/ai-saturdays/tf/lib/python2.7/site-packages/unidecode/__init__.pyc\u001b[0m in \u001b[0;36munidecode_expect_ascii\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_warn_if_not_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mbytestring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ASCII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xce in position 1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# Load Words\n",
    "# -*- coding: UTF-8 -*\n",
    "WORDS = {}\n",
    "with open('data/' + LANG + '_50k.txt') as f:\n",
    "    for line in f:\n",
    "        if LANG == 'en':\n",
    "            WORDS[unidecode.unidecode(line.split(\" \")[0])] = int(line.split(\" \")[1])\n",
    "        else:\n",
    "            WORDS[line.split(\" \")[0]] = int(line.split(\" \")[1])\n",
    "WORDS = Counter(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    if word in WORDS:\n",
    "        return word\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    \n",
    "    if LANG == 'cz':\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    else:\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cycler(ABC):\n",
    "    \"\"\" Abstract cycler class \"\"\" \n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 labels,\n",
    "                 charClass,\n",
    "                 stats=\"NO Stats Provided\",\n",
    "                 slider=(60, 15),\n",
    "                 ctc=False,\n",
    "                 seq2seq=False,\n",
    "                 charRNN=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.charClass = charClass\n",
    "        self.slider = slider\n",
    "        self.totalChars = sum([len(l) for l in labels])\n",
    "        self.ctc = ctc\n",
    "        self.seq2seq = seq2seq\n",
    "        self.charRNN = charRNN\n",
    "        self.stats = stats\n",
    "        \n",
    "        self.evaluate()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def recogniseWord(self, img):\n",
    "        pass\n",
    "    \n",
    "    def countCorrect(self, pred, label, lower=False):\n",
    "        correct = 0\n",
    "        for i in range(min(len(pred), len(label))):\n",
    "            if ((not lower and pred[i] == label[i])\n",
    "                 or (lower and pred[i] == label.lower()[i])):\n",
    "                correct += 1\n",
    "                \n",
    "        return correct        \n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\" Evaluate accuracy of the word classification \"\"\"\n",
    "        print()\n",
    "        print(\"STATS:\", self.stats)\n",
    "        print(self.labels[1], ':', self.recogniseWord(self.images[1]))\n",
    "        start_time = time.time()\n",
    "        correctLetters = 0\n",
    "        correctWords = 0\n",
    "        correctWordsCorrection = 0\n",
    "        correctLettersCorrection = 0\n",
    "        for i in range(len(self.images)):\n",
    "            word = self.recogniseWord(self.images[i])\n",
    "            correctLetters += self.countCorrect(word,\n",
    "                                         self.labels[i])\n",
    "            # Correction works only for lower letters\n",
    "            correctLettersCorrection += self.countCorrect(correction(word.lower()),\n",
    "                                                       self.labels[i],\n",
    "                                                       lower=True)\n",
    "            # Words accuracy\n",
    "            if word == self.labels[i]:\n",
    "                correctWords += 1\n",
    "            if correction(word.lower()) == self.labels[i].lower():\n",
    "                correctWordsCorrection += 1\n",
    "\n",
    "        print(\"Correct/Total: %s / %s\" % (correctLetters, self.totalChars))\n",
    "        print(\"Letter Accuracy: %s %%\" % round(correctLetters/self.totalChars * 100, 4))\n",
    "        print(\"Letter Accuracy with Correction: %s %%\" % round(correctLettersCorrection/self.totalChars * 100, 4))\n",
    "        print(\"Word Accuracy: %s %%\" % round(correctWords/len(self.images) * 100, 4))\n",
    "        print(\"Word Accuracy with Correction: %s %%\" % round(correctWordsCorrection/len(self.images) * 100, 4))\n",
    "        print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCycler(Cycler):\n",
    "    \"\"\" Cycle through the words and recognise them \"\"\" \n",
    "    def recogniseWord(self, img):\n",
    "        slider = self.slider\n",
    "        \n",
    "        if self.ctc:\n",
    "            step = 2    # 10 for (60, 60) slider\n",
    "            img = cv2.copyMakeBorder(\n",
    "                img,\n",
    "                0, 0, self.slider[1]//2, self.slider[1]//2,\n",
    "                cv2.BORDER_CONSTANT,\n",
    "                value=[0, 0, 0])\n",
    "            img = extendImg(\n",
    "                img,\n",
    "                (img.shape[0], max(-(-img.shape[1] // step) * step, self.slider[1] + step)))\n",
    "            length = (img.shape[1]-slider[1]) // step\n",
    "            input_seq = np.zeros((1, length, slider[0] * slider[1]), dtype=np.float32)\n",
    "            input_seq[0][:] = [img[:, loc*step: loc*step + slider[1]].flatten()\n",
    "                             for loc in range(length)]\n",
    "            input_seq = input_seq.swapaxes(0, 1)\n",
    "            \n",
    "            pred = self.charClass.eval_feed({'inputs:0': input_seq,\n",
    "                                             'inputs_length:0': [length],\n",
    "                                             'keep_prob:0': 1})[0]\n",
    "            \n",
    "            word = ''\n",
    "            for i in pred:\n",
    "                if word == 0 and i != 0:\n",
    "                    break\n",
    "                else:\n",
    "                    word += idx2char(i)\n",
    "            \n",
    "        else:       \n",
    "            length = img.shape[1]//slider[1]\n",
    "\n",
    "            input_seq = np.zeros((1, length, slider[0] * slider[1]), dtype=np.float32)\n",
    "            input_seq[0][:] = [img[:, loc * slider[1]: (loc+1) * slider[1]].flatten()\n",
    "                               for loc in range(length)]                                \n",
    "            input_seq = input_seq.swapaxes(0, 1)\n",
    "\n",
    "\n",
    "            if self.seq2seq:\n",
    "                targets = np.zeros((1, 1), dtype=np.int32)  \n",
    "                pred = self.charClass.eval_feed({'encoder_inputs:0': input_seq,\n",
    "                                                 'encoder_inputs_length:0': [length],\n",
    "                                                 'decoder_targets:0': targets,\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            else:\n",
    "                targets = np.zeros((1, 1, 4096), dtype=np.int32)  \n",
    "                pred = self.charClass.eval_feed({'encoder_inputs:0': input_seq,\n",
    "                                                 'encoder_inputs_length:0': [length],\n",
    "                                                 'letter_targets:0': targets,\n",
    "                                                 'is_training:0': False,\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            word = ''\n",
    "            for i in pred:\n",
    "                if word == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    word += idx2char(i, True)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCycler(Cycler):\n",
    "    \"\"\" Cycle through the words and recognise them \"\"\" \n",
    "    def recogniseWord(self, img):\n",
    "        img = cv2.copyMakeBorder(img,\n",
    "                                 0, 0, 30, 30,\n",
    "                                 cv2.BORDER_CONSTANT,\n",
    "                                 value=[0, 0, 0])\n",
    "        gaps = charSeg.segmentation(img, RNN=True)\n",
    "        \n",
    "        chars = []\n",
    "        for i in range(len(gaps)-1):\n",
    "            char = img[:, gaps[i]:gaps[i+1]]\n",
    "            # TODO None type error after treshold\n",
    "            char, dim = letterNorm(char, is_thresh=True, dim=True)\n",
    "            # TODO Test different values\n",
    "            if dim[0] > 4 and dim[1] > 4:\n",
    "                chars.append(char.flatten())\n",
    "                \n",
    "        chars = np.array(chars)\n",
    "        word = ''\n",
    "        if len(chars) != 0:\n",
    "            if self.charRNN:\n",
    "                pred = self.charClass.eval_feed({'inputs:0': [chars],\n",
    "                                                 'length:0': [len(chars)],\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            else:\n",
    "                pred = self.charClass.run(chars)\n",
    "                \n",
    "            for c in pred:\n",
    "                # word += CHARS[charIdx]\n",
    "                word += idx2char(c)        \n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATS: Seq2Seq\n",
      "spreads : zpasobe\n",
      "Correct/Total: 626 / 1356\n",
      "Letter Accuracy: 46.1652 %\n",
      "Letter Accuracy with Correction: 45.8702 %\n",
      "Word Accuracy: 21.3483 %\n",
      "Word Accuracy with Correction: 29.588 %\n",
      "--- 28.33 seconds ---\n",
      "\n",
      "STATS: Seq2Seq2CNN\n",
      "spreads : spreadds\n",
      "Correct/Total: 830 / 1356\n",
      "Letter Accuracy: 61.2094 %\n",
      "Letter Accuracy with Correction: 61.2094 %\n",
      "Word Accuracy: 28.4644 %\n",
      "Word Accuracy with Correction: 44.1948 %\n",
      "--- 43.31 seconds ---\n",
      "\n",
      "STATS: CTC\n",
      "spreads : spreads\n",
      "Correct/Total: 853 / 1356\n",
      "Letter Accuracy: 62.9056 %\n",
      "Letter Accuracy with Correction: 67.1091 %\n",
      "Word Accuracy: 41.1985 %\n",
      "Word Accuracy with Correction: 56.5543 %\n",
      "--- 36.54 seconds ---\n",
      "\n",
      "STATS: Bi-RNN and CNN\n",
      "spreads : spreads\n",
      "Correct/Total: 1046 / 1356\n",
      "Letter Accuracy: 77.1386 %\n",
      "Letter Accuracy with Correction: 79.2773 %\n",
      "Word Accuracy: 63.2959 %\n",
      "Word Accuracy with Correction: 72.2846 %\n",
      "--- 65.27 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CharCycler at 0x7f1f01150198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class cycling through words\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass,\n",
    "           stats='Seq2Seq',\n",
    "           slider=(60, 2),\n",
    "           seq2seq=True)\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass2,\n",
    "           stats='Seq2Seq2CNN',\n",
    "           slider=(60, 2))\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass3,\n",
    "           stats='CTC',\n",
    "           slider=(60, 2),\n",
    "           ctc=True)\n",
    "\n",
    "CharCycler(images,\n",
    "           labels,\n",
    "           charClass_1,\n",
    "           stats='Bi-RNN and CNN',\n",
    "           charRNN=False)\n",
    "\n",
    "# Cycler(images,\n",
    "#        labels,\n",
    "#        charClass_2,\n",
    "#        charRNN=True)\n",
    "\n",
    "# Cycler(images,\n",
    "#        labels,\n",
    "#        charClass_3,\n",
    "#        charRNN=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
